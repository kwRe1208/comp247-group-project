{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project - KSI data - Classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Target Column***\n",
    "ACCLASS<br>\n",
    "Required to transform into binary (0, 1):<br>\n",
    "'Fatal' --> 1, <br>\n",
    "'Non-Fatal Injury' --> 0, <br>\n",
    "'Property Damage Only' --> 0, <br>\n",
    "***5 nan value from this column, we can consider to drop them***\n",
    "\n",
    "\n",
    "\n",
    "below columns need to fill values:\n",
    "'PEDESTRIAN', 'CYCLIST', 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK',\n",
    "'TRSN_CITY_VEH', 'EMERG_VEH', 'PASSENGER', 'SPEEDING', 'AG_DRIV',\n",
    "'REDLIGHT', 'ALCOHOL', 'DISABILITY'\n",
    "fill Nan as No, and transform to 0, 1\n",
    "(Default they are Yes, Nan values)\n",
    "ROAD_CLASS fill most freq value\n",
    "DISTRICT fill most freq value\n",
    "\n",
    "Questionable column:\n",
    "CYCCOND: multi categories, fill Nan as most freq value??\n",
    "\n",
    "\n",
    "From the dataset, below columns are unnecessary:\n",
    "ObjectId, HEIGHBOURHOOD_158, HEIGHBOURHOOD_140, CYCLISTYPE(too much categories and too much Nan value),<br>\n",
    "PEDCOND(too much categories and too much Nan value), PEDACT(too much categories and too much Nan value),<br>\n",
    "PEDTYPE (too much categories and too much Nan value), DRICOND ('other' included, means it is not a accuracy value), DRIVACT ('other' included, means it is not a accuracy value), MANOEUVER('other' included, means it is not a accuracy value)<br>\n",
    "FATAL_NO, INVTYPE, DATE, YEAR, ACCNUM, INDEX_, STREET1, STREET2, OFFSET, X, Y,INJURY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r'dataset\\KSI.csv'\n",
    "\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>INDEX_</th>\n",
       "      <th>ACCNUM</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>STREET1</th>\n",
       "      <th>STREET2</th>\n",
       "      <th>OFFSET</th>\n",
       "      <th>...</th>\n",
       "      <th>AG_DRIV</th>\n",
       "      <th>REDLIGHT</th>\n",
       "      <th>ALCOHOL</th>\n",
       "      <th>DISABILITY</th>\n",
       "      <th>HOOD_158</th>\n",
       "      <th>NEIGHBOURHOOD_158</th>\n",
       "      <th>HOOD_140</th>\n",
       "      <th>NEIGHBOURHOOD_140</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>ObjectId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-8.844611e+06</td>\n",
       "      <td>5.412414e+06</td>\n",
       "      <td>3387730</td>\n",
       "      <td>892658.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006/03/11 05:00:00+00</td>\n",
       "      <td>852</td>\n",
       "      <td>BLOOR ST W</td>\n",
       "      <td>DUNDAS ST W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>High Park North</td>\n",
       "      <td>88</td>\n",
       "      <td>High Park North (88)</td>\n",
       "      <td>D11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.844611e+06</td>\n",
       "      <td>5.412414e+06</td>\n",
       "      <td>3387731</td>\n",
       "      <td>892658.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006/03/11 05:00:00+00</td>\n",
       "      <td>852</td>\n",
       "      <td>BLOOR ST W</td>\n",
       "      <td>DUNDAS ST W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>High Park North</td>\n",
       "      <td>88</td>\n",
       "      <td>High Park North (88)</td>\n",
       "      <td>D11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.816480e+06</td>\n",
       "      <td>5.434843e+06</td>\n",
       "      <td>3388101</td>\n",
       "      <td>892810.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006/03/11 05:00:00+00</td>\n",
       "      <td>915</td>\n",
       "      <td>MORNINGSIDE AVE</td>\n",
       "      <td>SHEPPARD AVE E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146</td>\n",
       "      <td>Malvern East</td>\n",
       "      <td>132</td>\n",
       "      <td>Malvern (132)</td>\n",
       "      <td>D42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8.829728e+06</td>\n",
       "      <td>5.419071e+06</td>\n",
       "      <td>3389067</td>\n",
       "      <td>893184.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006/01/01 05:00:00+00</td>\n",
       "      <td>236</td>\n",
       "      <td>WOODBINE AVE</td>\n",
       "      <td>O CONNOR DR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>Woodbine-Lumsden</td>\n",
       "      <td>60</td>\n",
       "      <td>Woodbine-Lumsden (60)</td>\n",
       "      <td>D55</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8.816480e+06</td>\n",
       "      <td>5.434843e+06</td>\n",
       "      <td>3388102</td>\n",
       "      <td>892810.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006/03/11 05:00:00+00</td>\n",
       "      <td>915</td>\n",
       "      <td>MORNINGSIDE AVE</td>\n",
       "      <td>SHEPPARD AVE E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146</td>\n",
       "      <td>Malvern East</td>\n",
       "      <td>132</td>\n",
       "      <td>Malvern (132)</td>\n",
       "      <td>D42</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              X             Y   INDEX_    ACCNUM  YEAR  \\\n",
       "0 -8.844611e+06  5.412414e+06  3387730  892658.0  2006   \n",
       "1 -8.844611e+06  5.412414e+06  3387731  892658.0  2006   \n",
       "2 -8.816480e+06  5.434843e+06  3388101  892810.0  2006   \n",
       "3 -8.829728e+06  5.419071e+06  3389067  893184.0  2006   \n",
       "4 -8.816480e+06  5.434843e+06  3388102  892810.0  2006   \n",
       "\n",
       "                     DATE  TIME          STREET1         STREET2 OFFSET  ...  \\\n",
       "0  2006/03/11 05:00:00+00   852       BLOOR ST W     DUNDAS ST W    NaN  ...   \n",
       "1  2006/03/11 05:00:00+00   852       BLOOR ST W     DUNDAS ST W    NaN  ...   \n",
       "2  2006/03/11 05:00:00+00   915  MORNINGSIDE AVE  SHEPPARD AVE E    NaN  ...   \n",
       "3  2006/01/01 05:00:00+00   236     WOODBINE AVE     O CONNOR DR    NaN  ...   \n",
       "4  2006/03/11 05:00:00+00   915  MORNINGSIDE AVE  SHEPPARD AVE E    NaN  ...   \n",
       "\n",
       "  AG_DRIV REDLIGHT  ALCOHOL  DISABILITY  HOOD_158 NEIGHBOURHOOD_158 HOOD_140  \\\n",
       "0     Yes      NaN      NaN         NaN        88   High Park North       88   \n",
       "1     Yes      NaN      NaN         NaN        88   High Park North       88   \n",
       "2     Yes      Yes      NaN         NaN       146      Malvern East      132   \n",
       "3     Yes      NaN      Yes         NaN        60  Woodbine-Lumsden       60   \n",
       "4     Yes      Yes      NaN         NaN       146      Malvern East      132   \n",
       "\n",
       "       NEIGHBOURHOOD_140 DIVISION ObjectId  \n",
       "0   High Park North (88)      D11        1  \n",
       "1   High Park North (88)      D11        2  \n",
       "2          Malvern (132)      D42        3  \n",
       "3  Woodbine-Lumsden (60)      D55        4  \n",
       "4          Malvern (132)      D42        5  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows which YEAR is 2020 or 2021 (COVID-19 pandemic)\n",
    "#df = df[~df['YEAR'].isin([2020, 2021])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows which has duplicate ACCNUM, same ACCNUM means same accident\n",
    "#df = df.drop_duplicates(subset='ACCNUM', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration <br>\n",
    "Use below code to display categrial data and null counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROAD_CLASS\n",
      "Major Arterial         12956\n",
      "Minor Arterial          2840\n",
      "Collector                996\n",
      "Local                    841\n",
      "Expressway               132\n",
      "Other                     25\n",
      "Laneway                   11\n",
      "Expressway Ramp            9\n",
      "Pending                    7\n",
      "Major Arterial Ramp        1\n",
      "Name: count, dtype: int64\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "print(df['ROAD_CLASS'].value_counts())\n",
    "print(df['ROAD_CLASS'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18194 entries, 0 to 18193\n",
      "Data columns (total 57 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   X                  18194 non-null  float64\n",
      " 1   Y                  18194 non-null  float64\n",
      " 2   INDEX_             18194 non-null  int64  \n",
      " 3   ACCNUM             13264 non-null  float64\n",
      " 4   YEAR               18194 non-null  int64  \n",
      " 5   DATE               18194 non-null  object \n",
      " 6   TIME               18194 non-null  int64  \n",
      " 7   STREET1            18194 non-null  object \n",
      " 8   STREET2            16510 non-null  object \n",
      " 9   OFFSET             3402 non-null   object \n",
      " 10  ROAD_CLASS         17818 non-null  object \n",
      " 11  DISTRICT           18089 non-null  object \n",
      " 12  WARDNUM            17332 non-null  float64\n",
      " 13  LATITUDE           18194 non-null  float64\n",
      " 14  LONGITUDE          18194 non-null  float64\n",
      " 15  LOCCOORD           18099 non-null  object \n",
      " 16  ACCLOC             12744 non-null  object \n",
      " 17  TRAFFCTL           18160 non-null  object \n",
      " 18  VISIBILITY         18174 non-null  object \n",
      " 19  LIGHT              18194 non-null  object \n",
      " 20  RDSFCOND           18169 non-null  object \n",
      " 21  ACCLASS            18189 non-null  object \n",
      " 22  IMPACTYPE          18190 non-null  object \n",
      " 23  INVTYPE            18178 non-null  object \n",
      " 24  INVAGE             18194 non-null  object \n",
      " 25  INJURY             9627 non-null   object \n",
      " 26  FATAL_NO           827 non-null    float64\n",
      " 27  INITDIR            13142 non-null  object \n",
      " 28  VEHTYPE            14966 non-null  object \n",
      " 29  MANOEUVER          10534 non-null  object \n",
      " 30  DRIVACT            9243 non-null   object \n",
      " 31  DRIVCOND           9240 non-null   object \n",
      " 32  PEDTYPE            3060 non-null   object \n",
      " 33  PEDACT             3082 non-null   object \n",
      " 34  PEDCOND            3084 non-null   object \n",
      " 35  CYCLISTYPE         774 non-null    object \n",
      " 36  CYCACT             766 non-null    object \n",
      " 37  CYCCOND            765 non-null    object \n",
      " 38  PEDESTRIAN         7354 non-null   object \n",
      " 39  CYCLIST            1906 non-null   object \n",
      " 40  AUTOMOBILE         16550 non-null  object \n",
      " 41  MOTORCYCLE         1587 non-null   object \n",
      " 42  TRUCK              1122 non-null   object \n",
      " 43  TRSN_CITY_VEH      1110 non-null   object \n",
      " 44  EMERG_VEH          43 non-null     object \n",
      " 45  PASSENGER          6906 non-null   object \n",
      " 46  SPEEDING           2575 non-null   object \n",
      " 47  AG_DRIV            9460 non-null   object \n",
      " 48  REDLIGHT           1520 non-null   object \n",
      " 49  ALCOHOL            788 non-null    object \n",
      " 50  DISABILITY         486 non-null    object \n",
      " 51  HOOD_158           18194 non-null  object \n",
      " 52  NEIGHBOURHOOD_158  18194 non-null  object \n",
      " 53  HOOD_140           18194 non-null  object \n",
      " 54  NEIGHBOURHOOD_140  18194 non-null  object \n",
      " 55  DIVISION           18194 non-null  object \n",
      " 56  ObjectId           18194 non-null  int64  \n",
      "dtypes: float64(7), int64(4), object(46)\n",
      "memory usage: 7.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['X', 'Y', 'INDEX_', 'ACCNUM', 'YEAR', 'DATE', 'TIME', 'STREET1',\n",
       "       'STREET2', 'OFFSET', 'ROAD_CLASS', 'DISTRICT', 'WARDNUM',\n",
       "       'LATITUDE', 'LONGITUDE', 'LOCCOORD', 'ACCLOC', 'TRAFFCTL',\n",
       "       'VISIBILITY', 'LIGHT', 'RDSFCOND', 'ACCLASS', 'IMPACTYPE',\n",
       "       'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO', 'INITDIR', 'VEHTYPE',\n",
       "       'MANOEUVER', 'DRIVACT', 'DRIVCOND', 'PEDTYPE', 'PEDACT', 'PEDCOND',\n",
       "       'CYCLISTYPE', 'CYCACT', 'CYCCOND', 'PEDESTRIAN', 'CYCLIST',\n",
       "       'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH',\n",
       "       'PASSENGER', 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL',\n",
       "       'DISABILITY', 'HOOD_158', 'NEIGHBOURHOOD_158', 'HOOD_140',\n",
       "       'NEIGHBOURHOOD_140', 'DIVISION', 'ObjectId'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since 5 rows are missing target values (ACCLASS), we will remove them\n",
    "df = df.dropna(subset=['ACCLASS'])\n",
    "\n",
    "#We will remove the columns that are not useful for our model\n",
    "meaningless_columns = ['INDEX_', 'ACCNUM', 'YEAR', 'DATE', 'TIME', 'STREET1',\n",
    "                       'STREET2', 'OFFSET', 'FATAL_NO', 'NEIGHBOURHOOD_158', 'NEIGHBOURHOOD_140',\n",
    "                       'ObjectId', 'WARDNUM', 'DIVISION']\n",
    "\n",
    "too_much_missing = ['PEDTYPE','CYCACT', 'CYCLISTYPE', 'PEDACT', 'CYCCOND', 'MANOEUVER', 'INITDIR']\n",
    "\n",
    "#We will remove the columns with duplicate information\n",
    "# X and Y are the same as LONGITUDE and LATITUDE\n",
    "# VEHTYPE, PEDCOND, DRIVCOND, IMPACTYPE, DRIVACT duplicated because there are categorical columns for the same information\n",
    "duplicated_columns = ['X', 'Y', 'VEHTYPE', 'PEDCOND', 'DRIVCOND', 'IMPACTYPE','LOCCOORD', 'HOOD_140', 'DRIVACT']\n",
    "\n",
    "\n",
    "#columns need to fill Nan values\n",
    "binary_map = {np.nan: 'No'}\n",
    "fill_nan_columns = ['PEDESTRIAN', 'CYCLIST', 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK',\n",
    "                    'TRSN_CITY_VEH', 'EMERG_VEH', 'PASSENGER', 'SPEEDING', 'AG_DRIV',\n",
    "                    'REDLIGHT', 'ALCOHOL', 'DISABILITY', 'INJURY']\n",
    "\n",
    "#columns which contain categorical data\n",
    "categorical_columns = ['LIGHT', 'INVAGE', 'RDSFCOND', \n",
    "                       'DISTRICT', 'ROAD_CLASS', 'TRAFFCTL', \n",
    "                       'ACCLOC', 'VISIBILITY','INVTYPE']\n",
    "\n",
    "try_exclude = ['ROAD_CLASS', 'TRAFFCTL', 'ACCLOC']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "df_origin = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop meaningless columns, duplicated columns and columns with too much missing values\n",
    "df = df.drop(columns=meaningless_columns)\n",
    "df = df.drop(columns=duplicated_columns)\n",
    "df = df.drop(columns=too_much_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with columns which contains many catefories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simplfy the categorical data\n",
    "# LIGHT\n",
    "# Daylight                10385\n",
    "# Dark                     3687\n",
    "# Dark, artificial         3300\n",
    "# Dusk                      240\n",
    "# Dusk, artificial          219\n",
    "# Daylight, artificial      141\n",
    "# Dawn                      110\n",
    "# Dawn, artificial          101\n",
    "# Other                       6\n",
    "\n",
    "#We will simplify the LIGHT column to Daylight, Dark, Dusk, Dawn, Other\n",
    "light_map = {\n",
    "    'Daylight': 'Daylight', \n",
    "    'Dark': 'Dark', \n",
    "    'Dark, artificial': 'Dark', \n",
    "    'Dusk': 'Dusk', \n",
    "    'Dusk, artificial': 'Dusk',         \n",
    "    'Daylight, artificial': 'Daylight', \n",
    "    'Dawn': 'Dawn', \n",
    "    'Dawn, artificial': 'Dawn', \n",
    "    'Other': 'Other'\n",
    "    }\n",
    "\n",
    "df['LIGHT'] = df['LIGHT'].map(light_map)\n",
    "\n",
    "# IMPACTYPE\n",
    "# Pedestrian Collisions     7293\n",
    "# Turning Movement          2792\n",
    "# Cyclist Collisions        1795\n",
    "# Rear End                  1746\n",
    "# SMV Other                 1457\n",
    "# Angle                     1283\n",
    "# Approaching                928\n",
    "# Sideswipe                  506\n",
    "# Other                      195\n",
    "# SMV Unattended Vehicle     190\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 4\n",
    "\n",
    "#Pedestrian, Cyclist are representing IMPACTYPE column, so we will drop it\n",
    "\n",
    "# INVAGE\n",
    "# unknown     2609\n",
    "# 20 to 24    1710\n",
    "# 25 to 29    1638\n",
    "# 30 to 34    1384\n",
    "# 35 to 39    1311\n",
    "# 50 to 54    1302\n",
    "# 40 to 44    1274\n",
    "# 45 to 49    1239\n",
    "# 55 to 59    1098\n",
    "# 60 to 64     877\n",
    "# 15 to 19     852\n",
    "# 65 to 69     681\n",
    "# 70 to 74     529\n",
    "# 75 to 79     434\n",
    "# 80 to 84     336\n",
    "# 10 to 14     249\n",
    "# 85 to 89     212\n",
    "# 5 to 9       199\n",
    "# 0 to 4       177\n",
    "# 90 to 94      63\n",
    "# Over 95       15\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 0\n",
    "\n",
    "#We will simplify the INVAGE column to 0 to 20, 20 to 40, 40 to 60, 60 to 80, over 80\n",
    "invage_map = {\n",
    "    'unknown': 'unknown',\n",
    "    '20 to 24': '20 to 39',\n",
    "    '25 to 29': '20 to 39',\n",
    "    '30 to 34': '20 to 39',\n",
    "    '35 to 39': '20 to 39',\n",
    "    '50 to 54': '40 to 59',\n",
    "    '40 to 44': '40 to 59',\n",
    "    '45 to 49': '40 to 59',\n",
    "    '55 to 59': '40 to 59',\n",
    "    '60 to 64': '60 to 79',\n",
    "    '15 to 19': '0 to 19',\n",
    "    '65 to 69': '60 to 79',\n",
    "    '70 to 74': '60 to 79',\n",
    "    '75 to 79': '60 to 79',\n",
    "    '80 to 84': 'over 79',\n",
    "    '10 to 14': '0 to 19',\n",
    "    '85 to 89': 'over 79',\n",
    "    '5 to 9': '0 to 19',\n",
    "    '0 to 4': '0 to 19',\n",
    "    '90 to 94': 'over 79',\n",
    "    'Over 95': 'over 79'\n",
    "    }\n",
    "\n",
    "df['INVAGE'] = df['INVAGE'].map(invage_map)\n",
    "\n",
    "# RDSFCOND\n",
    "# Dry                     14594\n",
    "# Wet                      3021\n",
    "# Loose Snow                169\n",
    "# Other                     145\n",
    "# Slush                     102\n",
    "# Ice                        77\n",
    "# Packed Snow                44\n",
    "# Loose Sand or Gravel       11\n",
    "# Spilled liquid              1\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 25\n",
    "\n",
    "#We will simplify the RDSFCOND column to Dry, Wet, Snow, Ice, Other\n",
    "rdsfcond_map = {\n",
    "    'Dry': 'Dry',\n",
    "    'Wet': 'Wet',\n",
    "    'Loose Snow': 'Wet',\n",
    "    'Other': 'Other',\n",
    "    'Slush': 'Wet',\n",
    "    'Ice': 'Wet',\n",
    "    'Packed Snow': 'Wet',\n",
    "    'Loose Sand or Gravel': 'Other',\n",
    "    'Spilled liquid': 'Other'\n",
    "    }\n",
    "\n",
    "df['RDSFCOND'] = df['RDSFCOND'].map(rdsfcond_map)\n",
    "\n",
    "# fill the missing values with other\n",
    "df['RDSFCOND'] = df['RDSFCOND'].fillna('Other')\n",
    "\n",
    "# DISTRICT\n",
    "# Toronto and East York    6125\n",
    "# Etobicoke York           4207\n",
    "# Scarborough              4111\n",
    "# North York               3637\n",
    "# Toronto East York           4\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 105\n",
    "\n",
    "# DISTRICT column has 105 missing values, we will fill them with the most frequent value\n",
    "df['DISTRICT'] = df['DISTRICT'].fillna(df['DISTRICT'].mode()[0])\n",
    "\n",
    "# DRIVACT\n",
    "# Driving Properly                4221\n",
    "# Failed to Yield Right of Way    1541\n",
    "# Lost control                     975\n",
    "# Improper Turn                    573\n",
    "# Other                            504\n",
    "# Disobeyed Traffic Control        475\n",
    "# Following too Close              251\n",
    "# Exceeding Speed Limit            246\n",
    "# Speed too Fast For Condition     208\n",
    "# Improper Lane Change             122\n",
    "# Improper Passing                 112\n",
    "# Wrong Way on One Way Road          9\n",
    "# Speed too Slow                     4\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 8948\n",
    "\n",
    "#Redlight, Speeding, Ag_Driv, Alcohol, Disability are representing DRIVACT column, so we will drop it\n",
    "\n",
    "# INITDIR\n",
    "# East       3259\n",
    "# West       3197\n",
    "# South      3106\n",
    "# North      3066\n",
    "# Unknown     510\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 5051\n",
    "\n",
    "# INITDIR column has 5051 missing values, we will drop it\n",
    "\n",
    "# ROAD_CLASS\n",
    "# Major Arterial         12951\n",
    "# Minor Arterial          2840\n",
    "# Collector                996\n",
    "# Local                    841\n",
    "# Expressway               132\n",
    "# Other                     25\n",
    "# Laneway                   11\n",
    "# Expressway Ramp            9\n",
    "# Pending                    7\n",
    "# Major Arterial Ramp        1\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 376\n",
    "\n",
    "# Simplify the ROAD_CLASS column to Major Arterial, Minor Arterial, Collector, Local, Other\n",
    "road_class_map = {\n",
    "    'Major Arterial': 'Major Arterial',\n",
    "    'Minor Arterial': 'Minor Arterial',\n",
    "    'Collector': 'Collector',\n",
    "    'Local': 'Local',\n",
    "    'Expressway': 'Other',\n",
    "    'Other': 'Other',\n",
    "    'Laneway': 'Other',\n",
    "    'Expressway Ramp': 'Other',\n",
    "    'Pending': 'Other',\n",
    "    'Major Arterial Ramp': 'Other'\n",
    "    }\n",
    "\n",
    "df['ROAD_CLASS'] = df['ROAD_CLASS'].map(road_class_map)\n",
    "\n",
    "# Fill the missing values with Other\n",
    "df['ROAD_CLASS'] = df['ROAD_CLASS'].fillna('Other')\n",
    "\n",
    "# TRAFFCTL\n",
    "# No Control              8788\n",
    "# Traffic Signal          7635\n",
    "# Stop Sign               1380\n",
    "# Pedestrian Crossover     198\n",
    "# Traffic Controller       108\n",
    "# Yield Sign                21\n",
    "# Streetcar (Stop for)      16\n",
    "# Traffic Gate               5\n",
    "# School Guard               2\n",
    "# Police Control             2\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 34\n",
    "\n",
    "# Simplyfy the TRAFFCTL column to No Control, Traffic Signal, Stop Sign, Other\n",
    "traffctl_map = {\n",
    "    'No Control': 'No Control',\n",
    "    'Traffic Signal': 'Traffic Signal',\n",
    "    'Stop Sign': 'Stop Sign',\n",
    "    'Pedestrian Crossover': 'Other',\n",
    "    'Traffic Controller': 'Other',\n",
    "    'Yield Sign': 'Other',\n",
    "    'Streetcar (Stop for)': 'Other',\n",
    "    'Traffic Gate': 'Other',\n",
    "    'School Guard': 'Other',\n",
    "    'Police Control': 'Other'\n",
    "    }\n",
    "\n",
    "df['TRAFFCTL'] = df['TRAFFCTL'].map(traffctl_map)\n",
    "\n",
    "# Fill the missing values with Other\n",
    "df['TRAFFCTL'] = df['TRAFFCTL'].fillna('Other')\n",
    "\n",
    "# ACCLOC\n",
    "# At Intersection          8689\n",
    "# Non Intersection         2420\n",
    "# Intersection Related     1200\n",
    "# At/Near Private Drive     379\n",
    "# Overpass or Bridge         17\n",
    "# Laneway                    14\n",
    "# Private Driveway           13\n",
    "# Underpass or Tunnel         6\n",
    "# Trail                       1\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 5450\n",
    "\n",
    "# Simplyfy the ACCLOC column to At Intersection, Non Intersection, Other\n",
    "accloc_map = {\n",
    "    'At Intersection': 'At Intersection',\n",
    "    'Non Intersection': 'Non Intersection',\n",
    "    'Intersection Related': 'At Intersection',\n",
    "    'At/Near Private Drive': 'Other',\n",
    "    'Overpass or Bridge': 'Other',\n",
    "    'Laneway': 'Other',\n",
    "    'Private Driveway': 'Other',\n",
    "    'Underpass or Tunnel': 'Other',\n",
    "    'Trail': 'Other'\n",
    "    }   \n",
    "\n",
    "df['ACCLOC'] = df['ACCLOC'].map(accloc_map)\n",
    "\n",
    "# Fill the missing values with Other\n",
    "df['ACCLOC'] = df['ACCLOC'].fillna('Other')\n",
    "\n",
    "# VISIBILITY\n",
    "# Clear                     15714\n",
    "# Rain                       1879\n",
    "# Snow                        351\n",
    "# Other                        97\n",
    "# Fog, Mist, Smoke, Dust       50\n",
    "# Freezing Rain                47\n",
    "# Drifting Snow                21\n",
    "# Strong wind                  10\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 20\n",
    "\n",
    "# Simplyfy the VISIBILITY column to Clear, Rain, Snow, Other\n",
    "\n",
    "visibility_map = {\n",
    "    'Clear': 'Clear',\n",
    "    'Rain': 'Not Clear',\n",
    "    'Snow': 'Not Clear',\n",
    "    'Other': 'Other',\n",
    "    'Fog, Mist, Smoke, Dust': 'Not Clear',\n",
    "    'Freezing Rain': 'Not Clear',\n",
    "    'Drifting Snow': 'Not Clear',\n",
    "    'Strong wind': 'Other'\n",
    "    }\n",
    "\n",
    "df['VISIBILITY'] = df['VISIBILITY'].map(visibility_map)\n",
    "\n",
    "# Fill the missing values with Other\n",
    "df['VISIBILITY'] = df['VISIBILITY'].fillna('Other')\n",
    "\n",
    "# INVTYPE\n",
    "# Driver                  8274\n",
    "# Pedestrian              3110\n",
    "# Passenger               2766\n",
    "# Vehicle Owner           1637\n",
    "# Cyclist                  784\n",
    "# Motorcycle Driver        697\n",
    "# Truck Driver             346\n",
    "# Other Property Owner     257\n",
    "# Other                    186\n",
    "# Motorcycle Passenger      39\n",
    "# Moped Driver              30\n",
    "# Driver - Not Hit          17\n",
    "# Wheelchair                17\n",
    "# In-Line Skater             5\n",
    "# Cyclist Passenger          3\n",
    "# Trailer Owner              2\n",
    "# Pedestrian - Not Hit       1\n",
    "# Witness                    1\n",
    "# Moped Passenger            1\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 16\n",
    "\n",
    "# Simplyfy the INVTYPE column to Driver, Pedestrian, Passenger, Vehicle Owner, Cyclist, Other\n",
    "\n",
    "invtype_map = {\n",
    "    'Driver': 'Driver',\n",
    "    'Pedestrian': 'Pedestrian',\n",
    "    'Passenger': 'Passenger',\n",
    "    'Vehicle Owner': 'Vehicle Owner',\n",
    "    'Cyclist': 'Cyclist',\n",
    "    'Motorcycle Driver': 'Driver',\n",
    "    'Truck Driver': 'Driver',\n",
    "    'Other Property Owner': 'Other',\n",
    "    'Other': 'Other',\n",
    "    'Motorcycle Passenger': 'Passenger',\n",
    "    'Moped Driver': 'Other',\n",
    "    'Driver - Not Hit': 'Other',\n",
    "    'Wheelchair': 'Other',\n",
    "    'In-Line Skater': 'Other',\n",
    "    'Cyclist Passenger': 'Passenger',\n",
    "    'Trailer Owner': 'Vehicle Owner',\n",
    "    'Pedestrian - Not Hit': 'Other',\n",
    "    'Witness': 'Other',\n",
    "    'Moped Passenger': 'Passenger'\n",
    "    }\n",
    "\n",
    "df['INVTYPE'] = df['INVTYPE'].map(invtype_map)\n",
    "\n",
    "# Fill the missing values with Other\n",
    "df['INVTYPE'] = df['INVTYPE'].fillna('Other')\n",
    "\n",
    "# MANOEUVER\n",
    "# Going Ahead                            6265\n",
    "# Turning Left                           1786\n",
    "# Stopped                                 620\n",
    "# Turning Right                           476\n",
    "# Slowing or Stopping                     282\n",
    "# Changing Lanes                          216\n",
    "# Parked                                  183\n",
    "# Other                                   181\n",
    "# Reversing                               122\n",
    "# Unknown                                 122\n",
    "# Making U Turn                           106\n",
    "# Overtaking                               91\n",
    "# Pulling Away from Shoulder or Curb       40\n",
    "# Pulling Onto Shoulder or towardCurb      18\n",
    "# Merging                                  18\n",
    "# Disabled                                  4\n",
    "# Name: count, dtype: int64\n",
    "# Null Values: 7659\n",
    "\n",
    "# Too difficult to simplify and too much missing values, we will drop it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ROAD_CLASS' 'DISTRICT' 'LATITUDE' 'LONGITUDE' 'ACCLOC' 'TRAFFCTL'\n",
      " 'VISIBILITY' 'LIGHT' 'RDSFCOND' 'ACCLASS' 'INVTYPE' 'INVAGE' 'INJURY'\n",
      " 'PEDESTRIAN' 'CYCLIST' 'AUTOMOBILE' 'MOTORCYCLE' 'TRUCK' 'TRSN_CITY_VEH'\n",
      " 'EMERG_VEH' 'PASSENGER' 'SPEEDING' 'AG_DRIV' 'REDLIGHT' 'ALCOHOL'\n",
      " 'DISABILITY' 'HOOD_158']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.values)\n",
    "print(len(df.columns.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LIGHT', 'INVAGE', 'RDSFCOND', 'DISTRICT', 'ROAD_CLASS', 'TRAFFCTL', 'ACCLOC', 'VISIBILITY', 'INVTYPE']\n"
     ]
    }
   ],
   "source": [
    "# columns can try to exclude or include\n",
    "\n",
    "#selected_categorical_columns = set(categorical_columns) - set(try_exclude)\n",
    "selected_categorical_columns = categorical_columns\n",
    "print(selected_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LIGHT', 'INVAGE', 'RDSFCOND', 'DISTRICT', 'ROAD_CLASS', 'TRAFFCTL', 'ACCLOC', 'VISIBILITY', 'INVTYPE', 'PEDESTRIAN', 'CYCLIST', 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH', 'PASSENGER', 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY', 'INJURY']\n"
     ]
    }
   ],
   "source": [
    "# join selected_categorical_columns and fill_nan_columns\n",
    "selected_cat_columns = list(selected_categorical_columns) + fill_nan_columns\n",
    "\n",
    "print(selected_cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(columns=try_exclude, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[fill_nan_columns] = df[fill_nan_columns].fillna(value='No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18189 entries, 0 to 18193\n",
      "Data columns (total 27 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   ROAD_CLASS     18189 non-null  object \n",
      " 1   DISTRICT       18189 non-null  object \n",
      " 2   LATITUDE       18189 non-null  float64\n",
      " 3   LONGITUDE      18189 non-null  float64\n",
      " 4   ACCLOC         18189 non-null  object \n",
      " 5   TRAFFCTL       18189 non-null  object \n",
      " 6   VISIBILITY     18189 non-null  object \n",
      " 7   LIGHT          18189 non-null  object \n",
      " 8   RDSFCOND       18189 non-null  object \n",
      " 9   ACCLASS        18189 non-null  object \n",
      " 10  INVTYPE        18189 non-null  object \n",
      " 11  INVAGE         18189 non-null  object \n",
      " 12  INJURY         18189 non-null  object \n",
      " 13  PEDESTRIAN     18189 non-null  object \n",
      " 14  CYCLIST        18189 non-null  object \n",
      " 15  AUTOMOBILE     18189 non-null  object \n",
      " 16  MOTORCYCLE     18189 non-null  object \n",
      " 17  TRUCK          18189 non-null  object \n",
      " 18  TRSN_CITY_VEH  18189 non-null  object \n",
      " 19  EMERG_VEH      18189 non-null  object \n",
      " 20  PASSENGER      18189 non-null  object \n",
      " 21  SPEEDING       18189 non-null  object \n",
      " 22  AG_DRIV        18189 non-null  object \n",
      " 23  REDLIGHT       18189 non-null  object \n",
      " 24  ALCOHOL        18189 non-null  object \n",
      " 25  DISABILITY     18189 non-null  object \n",
      " 26  HOOD_158       18189 non-null  object \n",
      "dtypes: float64(2), object(25)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the cleaned dataset\n",
    "df.to_csv(r'dataset\\KSI_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['ACCLASS'], axis=1)\n",
    "y = df['ACCLASS']\n",
    "\n",
    "data_map = {\n",
    "    'Fatal': 1, \n",
    "    'Non-Fatal Injury': 0, \n",
    "    'Property Damage Only': 0\n",
    "    }\n",
    "\n",
    "y = y.map(data_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROAD_CLASS        object\n",
       "DISTRICT          object\n",
       "LATITUDE         float64\n",
       "LONGITUDE        float64\n",
       "ACCLOC            object\n",
       "TRAFFCTL          object\n",
       "VISIBILITY        object\n",
       "LIGHT             object\n",
       "RDSFCOND          object\n",
       "ACCLASS           object\n",
       "INVTYPE           object\n",
       "INVAGE            object\n",
       "INJURY            object\n",
       "PEDESTRIAN        object\n",
       "CYCLIST           object\n",
       "AUTOMOBILE        object\n",
       "MOTORCYCLE        object\n",
       "TRUCK             object\n",
       "TRSN_CITY_VEH     object\n",
       "EMERG_VEH         object\n",
       "PASSENGER         object\n",
       "SPEEDING          object\n",
       "AG_DRIV           object\n",
       "REDLIGHT          object\n",
       "ALCOHOL           object\n",
       "DISABILITY        object\n",
       "HOOD_158          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18189, 71) (18189,)\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing for categorical data\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    \n",
    "])\n",
    "\n",
    "#Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', cat_transformer, selected_cat_columns),\n",
    "    ])\n",
    "\n",
    "#get transformed data\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "X_transformed = pd.DataFrame(X_transformed)\n",
    "print(X_transformed.shape, y.shape)\n",
    "\n",
    "#Resample the data\n",
    "smote = SMOTE(random_state=58)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_transformed, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACCLASS\n",
       "1    15616\n",
       "0    15616\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;model&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;model&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('model', LogisticRegression())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "#Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "#Preprocessing of training data, fit model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7811749639827117\n",
      "[[2521  601]\n",
      " [ 766 2359]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79      3122\n",
      "           1       0.80      0.75      0.78      3125\n",
      "\n",
      "    accuracy                           0.78      6247\n",
      "   macro avg       0.78      0.78      0.78      6247\n",
      "weighted avg       0.78      0.78      0.78      6247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing of validation data, get predictions\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "#Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "print('Accuracy:', accuracy_score(y_test, y_preds))\n",
    "\n",
    "print(confusion_matrix(y_test, y_preds))\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with different classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "{'model__C': 100, 'model__solver': 'liblinear'}\n",
      "Accuracy: 0.785336961741636\n"
     ]
    }
   ],
   "source": [
    "# Use SearchCV to find best estimator with Logistic Regression model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "y_preds = grid_search.predict(X_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_preds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'model__C': 100, 'model__kernel': 'rbf'}\n",
      "Accuracy: 0.9404514166800064\n"
     ]
    }
   ],
   "source": [
    "# Create SVM model, and use GridSearchCV to find best estimator\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "y_preds = grid_search.predict(X_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Random Forest model, and use GridSearchCV to find best estimator\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    'model__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'model__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "y_preds = grid_search.predict(X_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
